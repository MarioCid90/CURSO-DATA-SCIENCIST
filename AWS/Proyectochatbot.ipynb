{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioCid90/PRUEBA2/blob/main/AWS/Proyectochatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aYuVt6OKVtYT"
      },
      "outputs": [],
      "source": [
        "#importamos librerías\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import nltk #librería para trabajar con palabras\n",
        "from nltk.stem import WordNetLemmatizer # coge el array de cada paralabra\n",
        "from keras.models import Sequential #modelo secuancial\n",
        "from keras.layers import Dense, Activation, Dropout #capas de la red neuronal\n",
        "from keras.optimizers import SGD #optimizador para la red neuronal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dim4gMd8VvK4"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer() # llamamos al algoritmo para convertir las palabras a 1 y 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aw0lh2eYXmWk"
      },
      "outputs": [],
      "source": [
        "intents = json.loads(open('intents.json').read()) #cargamos el archivo json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYa-okJnaH7W",
        "outputId": "9d215845-760d-48da-ae23-799009d17f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#Estos modelos permiten dividir el texto en oraciones de manera efectiva.\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oKhNyeEFWuL-"
      },
      "outputs": [],
      "source": [
        "#Creamos listas\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = ['?','!','¿','.', ','] #No nos influyen estos simbolos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7azQP73uXkCb"
      },
      "outputs": [],
      "source": [
        "\n",
        "for intent in intents['intenciones']: #acceder a todos los patrones del json\n",
        "  for pattern in intent['texto']: #patrones que en json llamamos texto\n",
        "    word_list = nltk.word_tokenize(pattern) #facilita convertir todo a 1 y 0\n",
        "    words.extend(word_list) #La lista de palabras que hemos creado van a pasar por esta función\n",
        "    documents.append((word_list, intent['texto'])) #relaciona las palabras con el identificador\n",
        "    if intent['texto'] not in classes: #Si no hemos añadido el identificador\n",
        "      classes.append(intent['texto']) #que me las relacione igual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Vd86nBCgaxWK"
      },
      "outputs": [],
      "source": [
        "#LAs palabras que hemos separado de los patrones, las devolvemos a la palabra raiz\n",
        "#para que no haya confusiones a la hora de entrenarlo\n",
        "words= [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
        "#ordenamos las palabras y las convertimos a un SET\n",
        "words = sorted(set(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bDLYd1rAbWYI"
      },
      "outputs": [],
      "source": [
        "#Guardar las palabras\n",
        "\n",
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5cUzEsTDbnSG"
      },
      "outputs": [],
      "source": [
        "\n",
        "training = [] #lista de entrenamiento\n",
        "output_empty = [0] * len(classes) #crea tantos 0 como patrones tenemos\n",
        "\n",
        "for document in documents: #\n",
        "  bag = [] #lista con un monton de variables\n",
        "  word_patters = document[0] #todas las palabras que tenemos en la word_list\n",
        "  word_patters = [lemmatizer.lemmatize(word.lower()) for word in word_patters] #\n",
        "  for word in words: #para cada palabra en esta lista añadimos 1 y la palabra está en word_patters\n",
        "    bag.append(1) if word in word_patters else bag.append(0) #1 si la palabra pertenece al patrón\n",
        "#0 si la palabra no pertenece al patrón\n",
        "  output_row = list(output_empty) #variable que es la lista con todos los 1 y 0\n",
        "  output_row[classes.index(document[1])] = 1 #cogemos el indice de todas las clases = 1\n",
        "  training.append([bag, output_row]) #añadir a la lista final las palabras y la lista\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2BcUkfuecffG"
      },
      "outputs": [],
      "source": [
        "#dividimos todos los datos en 2 listas, una con los 0 y otra con los 1\n",
        "train_X=list(map(lambda x: x[0], training))\n",
        "train_Y=list(map(lambda x: x[1], training))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kqW2Os-wgaeO"
      },
      "outputs": [],
      "source": [
        "#Creamos capas de la red neuronal\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_X[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_Y[0]), activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K4956puPhMyK"
      },
      "outputs": [],
      "source": [
        "#vamos a definir el optimizador SGD\n",
        "\n",
        "sgd = SGD(learning_rate=0.001,  momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lzxp3UShmQL",
        "outputId": "023e1628-a936-4552-fd08-ca3e0909f92c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "83/83 [==============================] - 1s 2ms/step - loss: 3.6700 - accuracy: 0.0316\n",
            "Epoch 2/4\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 3.6544 - accuracy: 0.0364\n",
            "Epoch 3/4\n",
            "83/83 [==============================] - 0s 4ms/step - loss: 3.6397 - accuracy: 0.0583\n",
            "Epoch 4/4\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 3.6186 - accuracy: 0.0874\n"
          ]
        }
      ],
      "source": [
        "#Entrenamos el modelo\n",
        "train_proces =model.fit(np.array(train_X), np.array(train_Y), epochs=4, batch_size=5, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxycrMwbkpoB",
        "outputId": "7ac50b85-e496-48d3-b053-4b14f2ccec22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "#Guardamos en un archivo todo el proceso de entrenamiento\n",
        "\n",
        "model.save('chatbot_model.h5', train_proces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJnqeOY8h60d"
      },
      "outputs": [],
      "source": [
        "#importación de keras para cargar la data que acabamos de guardar\n",
        "from keras.models import load_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj7FUt9VjZk6"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer() #llamamos al algoritmo nuevamente\n",
        "intents = json.loads(open('intents.json').read()) #cargamos el archivo json\n",
        "words = pickle.load(open('words.pkl', 'rb'))\n",
        "classes = pickle.load(open('classes.pkl', 'rb'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGgPJa60jmSf"
      },
      "outputs": [],
      "source": [
        "#llamamos al modelo entrenado\n",
        "model = load_model('chatbot_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcqRweX6lLaf"
      },
      "outputs": [],
      "source": [
        "#Esta funcion devuelve lista de palabras bien organizadas\n",
        "def clean_up_sentence(sentence):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
        "  return sentence_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cr4VZqnlof2"
      },
      "outputs": [],
      "source": [
        "#Si una de las palabras está dentro de las palabras definidas, nos la transforma en 1\n",
        "#Si no a 0\n",
        "def bag_of_words(sentence):\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "  bag = [0] * len(words)\n",
        "  for w in sentence_words:\n",
        "    for i, word in enumerate(words):\n",
        "      if word == w:\n",
        "        bag[i] = 1\n",
        "  return np.array(bag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MAJlnw-l2FM"
      },
      "outputs": [],
      "source": [
        "#predice a la clase que pertenece la oración que escribimos.\n",
        "#devuelve el indice con más probabilidad de paracerse a la clase.\n",
        "def predict_class(sentence):\n",
        "  bow = bag_of_words(sentence)\n",
        "  res = model.predict(np.array([bow]))[0]\n",
        "  max_index = np.where(res == np.max(res))[0][0]\n",
        "  return classes[max_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs2FlqrjmmUz"
      },
      "outputs": [],
      "source": [
        "#Función para que nos devuelva una respuesta\n",
        "def get_response(intents_list, intents_json):\n",
        "  list_of_intents = intents_json['intenciones']\n",
        "  for i in list_of_intents:\n",
        "    if i['texto'] == intents_list:\n",
        "      result = random.choice(i['respuestas'])\n",
        "      break\n",
        "  return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUduZ52OGIUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43a65ad2-5cbf-4001-8af5-8c51ae8dbdb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hola\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "¡Qué bueno verte de nuevo!\n",
            " cursos\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Nuestra universidad ofrece tecnología de la información, ingeniería informática, ingeniería mecánica, ingeniería química, ingeniería civil e ingeniería externa\n",
            " horarios\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "¡La universidad está abierta de 8 am a 5 pm de lunes a sábado!\n",
            " instalaciones\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "El departamento de ingeniería de nuestra universidad ofrece un laboratorio de aire acondicionado completo con conexión a Internet, aula inteligente, auditorio, biblioteca y comedor\n",
            " contacto\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Puedes contactar al: NÚMERO\n",
            " como se llama el director\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "XYZ es el director de la universidad y si necesita ayuda, llame primero a su sucursal. Eso es más apropiado\n",
            " salir\n",
            "¡Hasta luego!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    message = input(\" \")\n",
        "\n",
        "    # Agrega una condición para salir del bucle\n",
        "    if message.lower() == 'salir':\n",
        "        print(\"¡Hasta luego!\")\n",
        "        break  # Sale del bucle while\n",
        "\n",
        "    ints = predict_class(message)\n",
        "    res = get_response(ints, intents)\n",
        "    print(res)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wN7AP-OjXcKFwFpgSPNfTTeESusYDCwi",
      "authorship_tag": "ABX9TyPxEGK4hAnyurx6Lb2bdVCh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}